# 关于一次数据插入的思考

>	获取到一个网站的sitemap,打算将里面的网址，权重和爬取频率入库，记录一下过程。
>	环境：MacOs 10.12.4,数据库：MySql
>	
>	用scrapy拿到sitemap,一共4K多个文件，每个文件解压后大小10M,里面包含5W个链接。
>	方案一：sublime text3
>	刚开始，sublime text3,command + shift + F,搜索文件夹内所有文件的内容，然后保存文件，结果，walk到第70多个会自动停止，可能跟它自身的缓冲区大小有关，略过
>	方案一缺点：看不到过程，系统占用大，容易死机
>	方案二：scrapy 爬取入库
>	一共2.4亿多个item,每秒入库只有100个，开始优化：
>>	通过观察，数据库连接的建立和关闭太频繁，是执行时间的消耗大户，用Mysql的batch解决，这里用的是pymysql的excutemany,将每个文件中的5W个item转化成一个二元列表，然后再入库，代替了一开始的每个item都yield.
>>  改进后，快了不少，但是每秒钟只能处理4个文件，cpu已经满载，硬盘IO每秒几十M,这个时候每秒钟入库的数据忘了，几万个？
>>	入库完成需要11多个小时，一觉醒来，还没好，硬盘先满了，导致scrapy在申请系统资源的时候阻塞，还好，顺便清理一下硬盘，把不必要的移动到移动硬盘去。
>>	看了一下数据表，太大，于是开始优化数据表，因为每个字段都是差不多固定长度的，缩减所有字段，因为是宽松模式，varchar字段，过长会截断，过短会以实际长度保存，数据表缩减，后续可以用数据压缩，以及更多的优化，现在的当务之急是每秒处理的sitemap文件太少了，继续优化。
>>	既然sitemap的文件模式是固定的，并没有突发情况，所以此时的瓶颈在xpath上，网页解析固然方便，但是速度还是慢了，改用re.
>>	re采用comile先编译正则，进一步加快速度。
>>	由于之前已经采集了一些数据,采用insert ignore into的方式插入数据库，现在从头爬取，瓶颈变成了每次出现重复值的时候的处理，删表观察，删不动，太大，先sudo su，建立硬链接，然后再进入mysql 去drop table,然后rm -f原始文件即可
>>	mysql 的batch中如果有一个出现重复导致无法入库，则全部不入库，因为整个加起来是一个事务，要么都做，要么都不做，所以加ignore来忽略出错的单条执行语句，sitemap中还是有重复数据的。
>>	mysql运行时间长可以看看是否有阻塞，kill掉

>> 发现mysql 有 LOAD XML LOCAL INFILE语句，下载mysql文档。阅读文档得知，需要同时配置客户端和服务器端的local file函数，但是pymysql没有读取到配置文件，在链接函数处加入 local_infile=True解决。

>> 速度提升明显，处理一个sitemap用时3秒，3秒5W个，仍显得慢，需要5+小时。

>> cpu和硬盘IO仍然没吃满，增加多核心多进程，速度提升。

>> 此时完成整个数据插入需要3小时。

>> 考虑到该数据库并无多少关联，也不需要频繁读写，改用MyISAM引擎。

>> 此时完成整个数据插入需要半小时。

>> 配置文件中的相关缓存之类已经设置加大，明面上的优化已经到头，接下来是要优化看不见的地方了。

>> 优化分两步，优化等待和优化执行,是需要优化其中一个还是两个，不得而知。另外还有CPU，IO等分析

>> 打开命令行，brew install sysbench

>> 使用redis

>> 使用hadoop

>> 硬盘空间不够或者嫌40G解压慢的，这里可以利用 建立实名管道和gzip，将文件解压的结果实时导入。这个尚未测试过。
